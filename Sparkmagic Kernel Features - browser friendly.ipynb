{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Jupyter integration - available at http://aka.ms/sparkmeetup\n",
    "by [@aggFTW](https://twitter.com/aggftw) & [@theseoafs](https://twitter.com/theseoafs) & [@kamhawy](https://twitter.com/kamhawy)\n",
    "\n",
    "![spark](images/spark-logo-trademark.png)\n",
    "\n",
    "# Spark Basics\n",
    "\n",
    "<a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.\n",
    "\n",
    "There are different ways to use Spark, from command line shells to compiling jars that you can run in a Spark cluster.\n",
    "\n",
    "However, because Spark is so fast, you can run it interactively from within a \"notebook\". Jupyter is one of many notebook servers and has been vetted by the data science community as one of the best ones out there.\n",
    "\n",
    "This is our proposition on how to integrate Spark with Jupyter. We hope you like it!\n",
    "\n",
    "# Architecture\n",
    "\n",
    "By default Jupyter notebook comes with a **Python2** kernel. A kernel is a program that runs and interprets your code. You can install two additional kernels that you can use with the Jupyter notebook. These are:\n",
    "\n",
    "1. **PySpark** (for applications written in Python). PySpark kernel exposes the Spark programming model to Python\n",
    "2. **Spark** (for applications written in Scala)\n",
    "\n",
    "The kernels use a completely Open Source library, [sparkmagic](https://github.com/jupyter-incubator/sparkmagic), to communicate with Spark clusters. Here's the rough architecture:\n",
    "\n",
    "![arch](images/arch.png)\n",
    "\n",
    "The Jupyter notebook uses the sparkmagic library to send Spark code to Livy, an open source REST server for Spark. When you execute a code cell in a PySpark notebook, it creates a Livy session to execute your code.\n",
    "\n",
    "You can use the `%%info` magic to display the Livy session information for your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint:\n",
      "\thttps://YOURCLUSTER.azurehdinsight.net/livy\n",
      "\n",
      "Current session ID number:\n",
      "\tNone\n",
      "\n",
      "Session configs:\n",
      "\t{u'executorCores': 2, 'kind': 'spark', u'driverMemory': u'1000M'}\n",
      "\n",
      "Info for endpoint:\n",
      "    Sessions:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features available with the new kernels\n",
    "\n",
    "## Notebook setup\n",
    "\n",
    "When using Spark and Pyspark kernel notebooks, there is no need to create a SparkContext, or a HiveContext; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkContext (sc)\n",
    "- HiveContext (sqlContext)\n",
    "\n",
    "To run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.\n",
    "\n",
    "## Help magic (%%help)\n",
    "\n",
    "This magic gives you information about the different supported magics in the Spark and PySpark kernels and the usage for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<table>\n",
       "  <tr>\n",
       "    <th>Magic</th>\n",
       "    <th>Example</th>\n",
       "    <th>Explanation</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>info</td>\n",
       "    <td>%%info</td>\n",
       "    <td>Outputs session information for the current Livy endpoint.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>cleanup</td>\n",
       "    <td>%%cleanup -f</td>\n",
       "    <td>Deletes all sessions for the current Livy endpoint, including this notebook's session. The force flag is mandatory.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>delete</td>\n",
       "    <td>%%delete -f -s 0</td>\n",
       "    <td>Deletes a session by number for the current Livy endpoint. Cannot delete this kernel's session.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>logs</td>\n",
       "    <td>%%logs</td>\n",
       "    <td>Outputs the current session's Livy logs.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>configure</td>\n",
       "    <td>%%configure -f {\"executorMemory\": \"1000M\", \"executorCores\": 4}</td>\n",
       "    <td>Configure the session creation parameters. The force flag is mandatory if a session has already been\n",
       "    created and the session will be dropped and recreated.<br/>Look at <a href=\"https://github.com/cloudera/livy#request-body\">\n",
       "    Livy's POST /sessions Request Body</a> for a list of valid parameters. Parameters must be passed in as a JSON string.</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>sql</td>\n",
       "    <td>%%sql -o tables -q<br/>SHOW TABLES</td>\n",
       "    <td>Executes a SQL query against the sqlContext.\n",
       "    Parameters:\n",
       "      <ul>\n",
       "        <li>-o VAR_NAME: The result of the query will be available in the %%local Python context as a\n",
       "          <a href=\"http://pandas.pydata.org/\">Pandas</a> dataframe.</li>\n",
       "        <li>-q: The magic will return None instead of the dataframe (no visualization).</li>\n",
       "        <li>-m METHOD: Sample method, either <tt>take</tt> or <tt>sample</tt>.</li>\n",
       "        <li>-n MAXROWS: The maximum number of rows of a SQL query that will be pulled from Livy to Jupyter.\n",
       "            If this number is negative, then the number of rows will be unlimited.</li>\n",
       "        <li>-r FRACTION: Fraction used for sampling.</li>\n",
       "      </ul>\n",
       "    </td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>local</td>\n",
       "    <td>%%local<br/>a = 1</td>\n",
       "    <td>All the code in subsequent lines will be executed locally. Code must be valid Python code.</td>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session configuration (%%configure)\n",
    " \n",
    "Use the `%%configure` magic to configure new or existing Livy sessions.\n",
    "* If a session is already running, you can change the configuration by using the `-f` argument with `%%configure` magic. This will delete the current session and recreate it with the applied configurations. If you don't provide the **-f** argument, an error will be displayed and no configuration changes will be applied.\n",
    "* If you haven't already started the session, then the **-f** argument is not mandatory. Even if you use it with a session that you are just creating, it will not delete any currently running sessions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These are some session attributes that can be used for configuration \n",
    "- **\"name\"**: Name of the application\n",
    "- **\"driverMemory\"**: Memory for driver (e.g. 1000M, 2G) \n",
    "- **\"executorMemory\"**: Memory for executor (e.g. 1000M, 2G) \n",
    "\n",
    "For more attributes for session configuration see <a href=\"https://github.com/cloudera/livy/tree/6fe1e80cfc72327c28107e0de20c818c1f13e027#post-sessions\" target=\"_blank\"> POST /sessions request body</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint:\n",
      "\thttps://YOURCLUSTER.azurehdinsight.net/livy\n",
      "\n",
      "Current session ID number:\n",
      "\tNone\n",
      "\n",
      "Session configs:\n",
      "\t{u'executorCores': 4, 'kind': 'spark', u'executorMemory': u'4G', u'name': u'remotesparkmagics-sample'}\n",
      "\n",
      "Info for endpoint:\n",
      "    Sessions:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%configure -f {\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"executorCores\":4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n",
      "Creating HiveContext as 'sqlContext'\n",
      "fruitsReversed: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:11"
     ]
    }
   ],
   "source": [
    "val fruits = sc.textFile(\"wasb:///example/data/fruits.txt\")\n",
    "val fruitsReversed = fruits.map((fruit) => fruit.reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic visualization of queries \n",
    "\n",
    "The sparkmagic kernels automatically visualizes the output of SparkSQL queries. You are given the option to choose between several different types of visualizations:\n",
    "- Table\n",
    "- Pie\n",
    "- Line \n",
    "- Area\n",
    "- Bar\n",
    "\n",
    ">**TIP**: You can take the first N rows or sample a percentage of rows. Look at the syntax with the `%%help` magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL magic (%%sql)\n",
    "\n",
    "The sparkmagic kernels support easy inline SQL queries that execute a SQL query against the `sqlContext`. The (`-o [Variable]`) argument persists the output of the SQL query as a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" target=\"_blank\">Pandas dataframe</a> on the Jupyter server (e.g. `-o tables` in the example below). This means it'll be available in the local mode which will be explained later. The output will be automatically visualized after you run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql -o platform_state_df -m sample -r 1.0 -n 200\n",
    "SELECT clientid, querytime, deviceplatform, querydwelltime, state\n",
    "FROM hivesampletable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![query_result](images/autoviz_bar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running locally (%%local)\n",
    "\n",
    "You can use the `%%local` magic to run your code locally on the Jupyter server, which is the headnode of the HDInsight cluster. Here's a typical use case for this scenario. \n",
    "\n",
    "By default, the output of any code snippet that you run from a Jupyter notebook is available within the context of the session that is persisted on the worker nodes. However, if you want to save a trip to the worker nodes for every computation, and all the data that you need for your computation is available locally on the Jupyter server node, you can use the `%%local` magic to run the code snippet on the Jupyter server. Typically, you would use `%%local` magic in conjunction with the `%%sql` magic with `-o` parameter. The `-o` parameter would persist the output of the SparkSQL query locally and then `%%local` magic would trigger the next set of code snippet to run locally against the output of the SQL or Hive queries that is persisted locally.\n",
    "\n",
    "> **TIPS**: \n",
    "> * Working against a locally persisted data is especially useful when you want visual representation of the results because it gives you the flexibility of using the visual representation libraries such as **matplotlib**. If you work directly against the data on the remote worker nodes, the output received through Livy is always text that limits the options of visual representation.\n",
    "\n",
    "\n",
    "> * If your dataset is large, it is considered a best practice to do your computation or number-crunching on the cluster or in the SQL query rather than in local mode.\n",
    "\n",
    "\n",
    "When you use `%%local` all subsequent lines in the cell will be executed locally, meaning it doesn't even have to go to the Spark cluster! The code in the cell must be valid Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'clientid', u'deviceplatform', u'querydwelltime', u'querytime',\n",
      "       u'state'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "%%local\n",
    "print(platform_state_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "from ipywidgets import Output\n",
    "from IPython.core.display import display\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "o = Output()\n",
    "display(o)\n",
    "\n",
    "comment = \"This graph has nothing to do with the query above, but it shows how you could visualize\"\\\n",
    "          \"data yourself by using the state column, for example.\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://plot.ly/~Dreamshot/5718/electoral-college-votes-by-us-state/.csv')\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "df.columns = [\"state\", \"votes\"] \n",
    "\n",
    "import plotly\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "df['text'] = df['state'] \n",
    "    \n",
    "data = [dict(\n",
    "    type='choropleth',\n",
    "    colorscale = scl,\n",
    "    autocolorscale = False,\n",
    "    locations = df['state'],\n",
    "    z = df['votes'].astype(float),\n",
    "    locationmode = 'USA-states',\n",
    "    text = df['text'],\n",
    "    hoverinfo = 'location+z',\n",
    "    marker = dict(\n",
    "        line = dict (\n",
    "            color = 'rgb(255,255,255)',\n",
    "            width = 2\n",
    "        )\n",
    "    ),\n",
    "    colorbar = dict(\n",
    "        title = \"Votes\"\n",
    "    )\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Device per state',\n",
    "    geo = dict(\n",
    "        scope='usa',\n",
    "        projection=dict( type='albers usa' ),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(255, 255, 255)'\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "with o:\n",
    "    init_notebook_mode()\n",
    "    plotly.offline.iplot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![map](images/map_us.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session logs (%%logs)\n",
    "\n",
    "You can get the logs of your current Livy session to debug any issues you encounter. From the logs, you can retrieve the YARN application id and then use the YARN UI to look at the YARN logs for that application id. Yarn is the resource manager for Spark. URL for Yarn UI is `https://<clusterdnsname>.azurehdinsight.net/yarnui`. For instructions on how to access the YARN UI for the cluster, see [Access YARN application logs on Linux-based HDInsight](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-access-yarn-app-logs-linux/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/03/09 22:59:53 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/03/09 22:59:54 WARN SparkConf: The configuration key 'spark.yarn.applicationMaster.waitTries' has been deprecated as of Spark 1.3 and and may be removed in the future. Please use the new key 'spark.yarn.am.waitTime' instead.\n",
      "16/03/09 22:59:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/09 22:59:55 INFO TimelineClientImpl: Timeline service address: http://hn0-YOURCLUSTER.dx.internal.cloudapp.net:8188/ws/v1/timeline/\n",
      "16/03/09 22:59:55 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\n",
      "16/03/09 22:59:55 INFO Client: Requesting a new application from cluster with 10 NodeManagers\n",
      "16/03/09 22:59:56 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (25600 MB per container)\n",
      "16/03/09 22:59:56 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n",
      "16/03/09 22:59:56 INFO Client: Setting up container launch context for our AM\n",
      "16/03/09 22:59:56 INFO Client: Setting up the launch environment for our AM container\n",
      "16/03/09 22:59:56 INFO Client: Preparing resources for our AM container\n",
      "16/03/09 22:59:56 INFO MetricsConfig: loaded properties from hadoop-metrics2-azure-file-system.properties\n",
      "16/03/09 22:59:56 INFO WasbAzureIaasSink: Init starting.\n",
      "16/03/09 22:59:56 INFO AzureIaasSink: Init starting. Initializing MdsLogger.\n",
      "16/03/09 22:59:56 INFO AzureIaasSink: Init completed.\n",
      "16/03/09 22:59:56 INFO WasbAzureIaasSink: Init completed.\n",
      "16/03/09 22:59:56 INFO MetricsSinkAdapter: Sink azurefs2 started\n",
      "16/03/09 22:59:56 INFO MetricsSystemImpl: Scheduled snapshot period at 60 second(s).\n",
      "16/03/09 22:59:56 INFO MetricsSystemImpl: azure-file-system metrics system started\n",
      "16/03/09 22:59:56 INFO Client: Uploading resource file:/usr/hdp/current/spark-client/conf/hive-site.xml -> wasb://yourcluster@storage.blob.core.windows.net/user/spark/.sparkStaging/application_1457486890867_0015/hive-site.xml\n",
      "16/03/09 22:59:57 INFO Client: Uploading resource file:/tmp/spark-337cb1fd-4b62-407a-8135-564cc29b461f/__spark_conf__8888428193921775437.zip -> wasb://yourcluster0@storage.blob.core.windows.net/user/spark/.sparkStaging/application_1457486890867_0015/__spark_conf__8888428193921775437.zip\n",
      "16/03/09 22:59:57 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n",
      "16/03/09 22:59:57 INFO SecurityManager: Changing view acls to: spark\n",
      "16/03/09 22:59:57 INFO SecurityManager: Changing modify acls to: spark\n",
      "16/03/09 22:59:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(spark); users with modify permissions: Set(spark)\n",
      "16/03/09 22:59:58 INFO Client: Submitting application 15 to ResourceManager\n",
      "16/03/09 22:59:58 INFO YarnClientImpl: Submitted application application_1457486890867_0015\n",
      "16/03/09 22:59:59 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 22:59:59 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1457564398030\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://yourcluster.dx.internal.cloudapp.net:8088/proxy/application_1457486890867_0015/\n",
      "\t user: spark\n",
      "16/03/09 23:00:00 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:01 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:02 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:03 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:04 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:05 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:06 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:07 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:08 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:09 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:10 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:11 INFO Client: Application report for application_1457486890867_0015 (state: ACCEPTED)\n",
      "16/03/09 23:00:12 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:12 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 10.0.0.9\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1457564398030\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://yourcluster.dx.internal.cloudapp.net:8088/proxy/application_1457486890867_0015/\n",
      "\t user: spark\n",
      "16/03/09 23:00:13 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:14 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:15 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:16 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:17 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:18 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:19 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:20 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:21 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:22 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:23 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:24 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:25 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:26 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:27 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:28 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:29 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:30 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:31 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:32 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:33 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:34 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:35 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:36 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:37 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:38 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:39 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:40 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:41 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:42 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:43 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:44 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:45 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:46 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:47 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:48 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:49 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:50 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)\n",
      "16/03/09 23:00:51 INFO Client: Application report for application_1457486890867_0015 (state: RUNNING)"
     ]
    }
   ],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete session (%%delete)\n",
    "\n",
    "Use '`%%delete -f -s <session #>`' to delete a session given its session ID. Note that you cannot delete the session that is initiated for the kernel itself. Another notebook might go into an error state, since notebooks are supposed to manage sessions by themselves, and all work will be lost on that session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions cleanup (%%cleanup)\n",
    "\n",
    "Use '`%%cleanup -f`' magic to delete all of the sessions for this cluster, including this notebook's session.\n",
    "The force flag `-f` is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We love feedback!\n",
    "\n",
    "### Useful links:\n",
    "\n",
    "* Contributions welcome at [sparkmagic repo](https://github.com/jupyter-incubator/sparkmagic)!\n",
    "* All Microsoft HDInsight Spark clusters come with Jupyter set up to be immediately productive on Spark. [Give it a try!](https://azure.microsoft.com/en-us/pricing/free-trial/)\n",
    "* [Feature Requests](https://feedback.azure.com/forums/217335-hdinsight)\n",
    "* [Forum Questions](https://social.msdn.microsoft.com/forums/azure/en-US/home?forum=hdinsight)\n",
    "* [hdinsight tag at StackOverflow](https://stackoverflow.com/questions/tagged/hdinsight)\n",
    "\n",
    "# Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
