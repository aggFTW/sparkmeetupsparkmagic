{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark and Jupyter integration\n",
    "by [@aggFTW](https://twitter.com/aggftw) & [@theseoafs](https://twitter.com/theseoafs) & [@kamhawy](https://twitter.com/kamhawy)\n",
    "\n",
    "![spark](images/spark-logo-trademark.png)\n",
    "\n",
    "# Spark Basics\n",
    "\n",
    "<a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.\n",
    "\n",
    "There are different ways to use Spark, from command line shells to compiling jars that you can run in a Spark cluster.\n",
    "\n",
    "However, because Spark is so fast, you can run it interactively from within a \"notebook\". Jupyter is one of many notebook servers and has been vetted by the data science community as one of the best ones out there.\n",
    "\n",
    "This is our proposition on how to integrate Spark with Jupyter. We hope you like it!\n",
    "\n",
    "# Architecture\n",
    "\n",
    "By default Jupyter notebook comes with a **Python2** kernel. A kernel is a program that runs and interprets your code. You can install two additional kernels that you can use with the Jupyter notebook. These are:\n",
    "\n",
    "1. **PySpark** (for applications written in Python). PySpark kernel exposes the Spark programming model to Python\n",
    "2. **Spark** (for applications written in Scala)\n",
    "\n",
    "The kernels use a completely Open Source library, [sparkmagic](https://github.com/jupyter-incubator/sparkmagic), to communicate with Spark clusters. Here's the rough architecture:\n",
    "\n",
    "![arch](images/arch.png)\n",
    "\n",
    "The Jupyter notebook uses the sparkmagic library to send Spark code to Livy, an open source REST server for Spark. When you execute a code cell in a PySpark notebook, it creates a Livy session to execute your code.\n",
    "\n",
    "You can use the `%%info` magic to display the Livy session information for your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features available with the new kernels\n",
    "\n",
    "## Notebook setup\n",
    "\n",
    "When using Spark and Pyspark kernel notebooks, there is no need to create a SparkContext, or a HiveContext; those are all created for you automatically when you run the first code cell, and you'll be able to see the progress printed. The contexts are created with the following variable names:\n",
    "- SparkContext (sc)\n",
    "- HiveContext (sqlContext)\n",
    "\n",
    "To run the cells below, place the cursor in the cell and then press **SHIFT + ENTER**.\n",
    "\n",
    "## Help magic (%%help)\n",
    "\n",
    "This magic gives you information about the different supported magics in the Spark and PySpark kernels and the usage for each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session configuration (%%configure)\n",
    " \n",
    "Use the `%%configure` magic to configure new or existing Livy sessions.\n",
    "* If a session is already running, you can change the configuration by using the `-f` argument with `%%configure` magic. This will delete the current session and recreate it with the applied configurations. If you don't provide the **-f** argument, an error will be displayed and no configuration changes will be applied.\n",
    "* If you haven't already started the session, then the **-f** argument is not mandatory. Even if you use it with a session that you are just creating, it will not delete any currently running sessions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "These are some session attributes that can be used for configuration \n",
    "- **\"name\"**: Name of the application\n",
    "- **\"driverMemory\"**: Memory for driver (e.g. 1000M, 2G) \n",
    "- **\"executorMemory\"**: Memory for executor (e.g. 1000M, 2G) \n",
    "\n",
    "For more attributes for session configuration see <a href=\"https://github.com/cloudera/livy/tree/6fe1e80cfc72327c28107e0de20c818c1f13e027#post-sessions\" target=\"_blank\"> POST /sessions request body</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%configure -f {\"name\":\"remotesparkmagics-sample\", \"executorMemory\": \"4G\", \"executorCores\":4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scala support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val fruits = sc.textFile(\"wasb:///example/data/fruits.txt\")\n",
    "val fruitsReversed = fruits.map((fruit) => fruit.reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic visualization of queries \n",
    "\n",
    "The sparkmagic kernels automatically visualizes the output of SparkSQL queries. You are given the option to choose between several different types of visualizations:\n",
    "- Table\n",
    "- Pie\n",
    "- Line \n",
    "- Area\n",
    "- Bar\n",
    "\n",
    ">**TIP**: You can take the first N rows or sample a percentage of rows. Look at the syntax with the `%%help` magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL magic (%%sql)\n",
    "\n",
    "The sparkmagic kernels support easy inline SQL queries that execute a SQL query against the `sqlContext`. The (`-o [Variable]`) argument persists the output of the SQL query as a <a href=\"http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\" target=\"_blank\">Pandas dataframe</a> on the Jupyter server (e.g. `-o tables` in the example below). This means it'll be available in the local mode which will be explained later. The output will be automatically visualized after you run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%sql -o platform_state_df -m sample -r 1.0 -n 200\n",
    "SELECT clientid, querytime, deviceplatform, querydwelltime, state\n",
    "FROM hivesampletable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running locally (%%local)\n",
    "\n",
    "You can use the `%%local` magic to run your code locally on the Jupyter server, which is the headnode of the HDInsight cluster. Here's a typical use case for this scenario. \n",
    "\n",
    "By default, the output of any code snippet that you run from a Jupyter notebook is available within the context of the session that is persisted on the worker nodes. However, if you want to save a trip to the worker nodes for every computation, and all the data that you need for your computation is available locally on the Jupyter server node, you can use the `%%local` magic to run the code snippet on the Jupyter server. Typically, you would use `%%local` magic in conjunction with the `%%sql` magic with `-o` parameter. The `-o` parameter would persist the output of the SparkSQL query locally and then `%%local` magic would trigger the next set of code snippet to run locally against the output of the SQL or Hive queries that is persisted locally.\n",
    "\n",
    "> **TIPS**: \n",
    "> * Working against a locally persisted data is especially useful when you want visual representation of the results because it gives you the flexibility of using the visual representation libraries such as **matplotlib**. If you work directly against the data on the remote worker nodes, the output received through Livy is always text that limits the options of visual representation.\n",
    "\n",
    "\n",
    "> * If your dataset is large, it is considered a best practice to do your computation or number-crunching on the cluster or in the SQL query rather than in local mode.\n",
    "\n",
    "\n",
    "When you use `%%local` all subsequent lines in the cell will be executed locally, meaning it doesn't even have to go to the Spark cluster! The code in the cell must be valid Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "print(platform_state_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%local\n",
    "from ipywidgets import Output\n",
    "from IPython.core.display import display\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "o = Output()\n",
    "display(o)\n",
    "\n",
    "comment = \"This graph has nothing to do with the query above, but it shows how you could visualize\"\\\n",
    "          \"data yourself by using the state column, for example.\"\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('https://plot.ly/~Dreamshot/5718/electoral-college-votes-by-us-state/.csv')\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "df.columns = [\"state\", \"votes\"] \n",
    "\n",
    "import plotly\n",
    "from plotly.graph_objs import *\n",
    "\n",
    "scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "            [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "\n",
    "df['text'] = df['state'] \n",
    "    \n",
    "data = [dict(\n",
    "    type='choropleth',\n",
    "    colorscale = scl,\n",
    "    autocolorscale = False,\n",
    "    locations = df['state'],\n",
    "    z = df['votes'].astype(float),\n",
    "    locationmode = 'USA-states',\n",
    "    text = df['text'],\n",
    "    hoverinfo = 'location+z',\n",
    "    marker = dict(\n",
    "        line = dict (\n",
    "            color = 'rgb(255,255,255)',\n",
    "            width = 2\n",
    "        )\n",
    "    ),\n",
    "    colorbar = dict(\n",
    "        title = \"Votes\"\n",
    "    )\n",
    ")]\n",
    "\n",
    "layout = dict(\n",
    "    title = 'Device per state',\n",
    "    geo = dict(\n",
    "        scope='usa',\n",
    "        projection=dict( type='albers usa' ),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(255, 255, 255)'\n",
    "    )\n",
    ")\n",
    "    \n",
    "fig = dict(data=data, layout=layout)\n",
    "\n",
    "with o:\n",
    "    init_notebook_mode()\n",
    "    plotly.offline.iplot(fig, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Session logs (%%logs)\n",
    "\n",
    "You can get the logs of your current Livy session to debug any issues you encounter. From the logs, you can retrieve the YARN application id and then use the YARN UI to look at the YARN logs for that application id. Yarn is the resource manager for Spark. URL for Yarn UI is `https://<clusterdnsname>.azurehdinsight.net/yarnui`. For instructions on how to access the YARN UI for the cluster, see [Access YARN application logs on Linux-based HDInsight](https://azure.microsoft.com/en-us/documentation/articles/hdinsight-hadoop-access-yarn-app-logs-linux/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete session (%%delete)\n",
    "\n",
    "Use '`%%delete -f -s <session #>`' to delete a session given its session ID. Note that you cannot delete the session that is initiated for the kernel itself. Another notebook might go into an error state, since notebooks are supposed to manage sessions by themselves, and all work will be lost on that session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sessions cleanup (%%cleanup)\n",
    "\n",
    "Use '`%%cleanup -f`' magic to delete all of the sessions for this cluster, including this notebook's session.\n",
    "The force flag `-f` is mandatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We love feedback!\n",
    "\n",
    "### Useful links:\n",
    "\n",
    "* Contributions welcome at [sparkmagic repo](https://github.com/jupyter-incubator/sparkmagic)!\n",
    "* All Microsoft HDInsight Spark clusters come with Jupyter set up to be immediately productive on Spark. [Give it a try!](https://azure.microsoft.com/en-us/pricing/free-trial/)\n",
    "* [Feature Requests](https://feedback.azure.com/forums/217335-hdinsight)\n",
    "* [Forum Questions](https://social.msdn.microsoft.com/forums/azure/en-US/home?forum=hdinsight)\n",
    "* [hdinsight tag at StackOverflow](https://stackoverflow.com/questions/tagged/hdinsight)\n",
    "\n",
    "# Thanks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
